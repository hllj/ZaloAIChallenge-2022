name: baseline
state: train # train/test/debug
seed: 6789

# path to original working directory
# hydra hijacks working directory by changing it to the current log directory,
# so it's useful to have this path as a special variable
# https://hydra.cc/docs/next/tutorials/basic/running_your_app/working_directory
work_dir: ${hydra:runtime.cwd}

logging:
  root: ./logs

defaults:
  - model: swin_large_patch4_window12_384
  - _self_

dataset:
  train_list: ${work_dir}/data/train/train6789.csv
  val_list: ${work_dir}/data/train/val6789.csv
  # val_list: ${work_dir}/data/vis_val/val.csv
  data_dir: ${work_dir}/data
  crop_size: 512
  batch_size: 16
  num_workers: 4
  pin_memory: True
  augmentation:
    aug0:
      _target_: torchvision.transforms.RandomResizedCrop
      size: ${dataset.crop_size}
      scale:
        - 0.49
        - 1.0
    aug1:
      _target_: torchvision.transforms.RandomVerticalFlip
      p: 0.2
    aug2:
      _target_: torchvision.transforms.GaussianBlur
      kernel_size: 3
      sigma:
        - 0.2
        - 2.0

trainer:
  # GPU related
  precision: 16
  accelerator: gpu
  devices: -1
  num_nodes: 1
  strategy: null #ddp if we want to use Multi-GPUs
  benchmark: True
  sync_batchnorm: False
  max_epochs: 20

# Logging, progress bar
refresh_rate: 20

model_ckpt:
  dirpath: ckpts/
  filename: "checkpoint-epoch{epoch}-step{step}-val_acc{val/acc:.3f}-val_loss{val/loss:.3f}"
  monitor: ${model.monitor}
  save_last: True
  save_top_k: 3
  mode: min
  auto_insert_metric_name: False

ddp_plugin:
  # These two args only work with accelerator = "ddp"
  find_unused_params: True # FIXME: Find out why turn this to False will fail to launch the training
  fp16_hook: True
  static_graph: False

hydra:
  run:
    dir: ./outputs3/${name}-${model.name}/${now:%Y-%m-%d-%H-%M-%S}
  sweep:
    dir: ./multirun/${name}-${model.name}
    subdir: ${now:%Y-%m-%d-%H-%M-%S}
  sweeper:
    params:
      dataset.augmentation.aug2.p: 0.0,0.1,0.2
      dataset.augmentation.aug3.brightness: 0.0,0.3
      dataset.augmentation.aug3.contrast: 0.0,0.3
      model.loss.label_smoothing: 0.0,0.1
      model.optimizer.lr: 1e-4,3e-4,3e-5,1e-5
      model.lr_scheduler.step_size: 5,10